{"cells":[{"cell_type":"markdown","metadata":{"id":"VdLbblbT3fIB"},"source":["# MAST-ML Workflow Activity\n","\n","\n","---\n","This activity serves as an introduction to the more pythonic way of working with the MAST-ML software. During the activity we'll see how to set up some basic MAST-ML workflows which mirror workflows previously explored via:\n","- Citrination\n","- Nanohub Introduction to Machine Learning for Materials Science\n","\n","Throughout the instructions we'll reference directly back to the Nanohub notebook. I'ld advise you have that open in a second tab to refer back to. you can find that notebook here:  \n","www.nanohub.org/tools/intromllab\n","\n","The overall goal is to reproduce those workflows using the MAST-ML software, learn how to execute calls to MAST-ML, and how to find and anlyze the results.\n","\n","This notebook is setup in a linear fashion where working from top to bottom will execute the full workflow.\n"]},{"cell_type":"markdown","metadata":{"id":"6UxJ6oB54hv9"},"source":["## Section 1: Setting up our Google Colab Environment\n","---\n","Before running any code we first need to install MAST-ML as well as it's dependencies into the colab environment.\n"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"eGlc1jSOh_SH"},"outputs":[],"source":["#pip install mastml"]},{"cell_type":"markdown","metadata":{"id":"myLD9v0lVVHe"},"source":["**Reminder, make sure to go to runtime at the top of the page and select \"restart session\" before continuing. After restarting the session you can skip the pip install command and proceed to the import and drive sync.**"]},{"cell_type":"markdown","metadata":{"id":"I93RLdVb5kl3"},"source":["Now we'll sync Colab with our google drive so that we can save directly our outputs to google drive. If you haven't already I recommend making a folder in google drive titled \"MASTML_colab\" or something similar to direct all your results towards. Going forward I'll assume this folder exists and I'll base the runs out of that folder. If you want to change the naming that can work as well as long as you update when that location is referenced."]},{"cell_type":"code","execution_count":35,"metadata":{"id":"w-jHgEAFquSh"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"Oe2e3F-x6EZz"},"source":["Here we import the MAST-ML modules used. Note that if you're making edits you may have to come back to update these imports to grab new functionality that isn't included here."]},{"cell_type":"code","execution_count":36,"metadata":{"id":"x8GIei1qQoWL"},"outputs":[],"source":["from mastml.mastml import Mastml\n","from mastml.datasets import LocalDatasets\n","from mastml.data_cleaning import DataCleaning\n","from mastml.preprocessing import SklearnPreprocessor\n","from mastml.models import SklearnModel\n","from mastml.data_splitters import SklearnDataSplitter, NoSplit\n","from mastml.feature_selectors import EnsembleModelFeatureSelector, NoSelect\n","from mastml.feature_generators import ElementalFeatureGenerator\n","from mastml.hyper_opt import GridSearch"]},{"cell_type":"markdown","metadata":{"id":"oyt61i7J6kmc"},"source":["And finally we'll import pandas to help with handling dataframes throughout the notebook."]},{"cell_type":"code","execution_count":37,"metadata":{"id":"QasqdefdEh7n"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"P7s6xyjq6zxv"},"source":["## Section 2: Data Cleaning\n","\n","\n","---\n","this section is largely the same as the previous notebook in functionality.\n","We'll read in the same initial bandgap data we used in the previous notebook then perform the same cleaning steps:  \n","1) Filtering for \"Reliability\"  \n","2) Averaging bandgap values where we have duplicates  \n"]},{"cell_type":"markdown","metadata":{"id":"AkmW1QuL8ci9"},"source":["Read in the band gap data from our dataset. If you haven't already upload the bandgap_data_v2.csv data to the MASTML_colab folder.\n","\n","If you uploaded previously to the default folder on colab you can now drag the .csv file into the MASTML_colab folder under /drive/MyDrive/MASTML_colab/"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"185olJA6EWgH"},"outputs":[],"source":["mastml_df = pd.read_csv(\"./bandgap_data_v2.csv\")"]},{"cell_type":"markdown","metadata":{"id":"PDkWyHHx8pSr"},"source":["Filter for only Reliability 1. This is the same python call used in the previous notebook."]},{"cell_type":"code","execution_count":41,"metadata":{"id":"G2STcYIzFl31"},"outputs":[],"source":["mastml_df_filtered = mastml_df[mastml_df[\"Reliability\"]==1]"]},{"cell_type":"markdown","metadata":{"id":"DE8ygBOx8tr-"},"source":["Note that for these steps we're not using MASTML because these are very custom steps to this dataset that aren't generic data cleaning steps included in MASTML."]},{"cell_type":"markdown","metadata":{"id":"pinx2EEU88jt"},"source":["We then do the same bandgap averaging when we have duplicates in the dataset."]},{"cell_type":"code","execution_count":42,"metadata":{"id":"dVywI4h5GpCF"},"outputs":[],"source":["mastml_df_clean = mastml_df_filtered.groupby(\"chemicalFormula Clean\", as_index = False).mean(numeric_only=True)"]},{"cell_type":"markdown","metadata":{"id":"2QtQtIOd9Cpd"},"source":["This section is new. We reset the index to match the previous notebook so that we can explicitly define the same Train / Test split that we used before. The test_indices object is just a hard copied list of the index values from the previous notebook. If you want to go check them you can find the X_test object and call X_test.index to see these yourself."]},{"cell_type":"code","execution_count":43,"metadata":{"id":"7kaM8janNfaH"},"outputs":[],"source":["mastml_df_clean.reset_index(inplace=True)\n","mastml_df_clean.drop(columns='level_0',inplace=True)"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"nDVtUAEyP7KO"},"outputs":[],"source":["test_indices = [279, 168, 192,  33, 223,  22, 341, 453, 460, 455, 120, 430, 436,\n","            366, 292, 278, 163, 216, 420, 210, 214, 422, 340,  41, 416, 146,\n","            280, 229, 300, 111, 407, 250, 379,  20, 356,   4, 141, 139, 121,\n","            324, 147, 415,  57, 301, 393, 454,  30]"]},{"cell_type":"markdown","metadata":{"id":"IQWI9X0A9e9D"},"source":["Finally we define a new column \"testdata\" which is going to be a binary column that is either 0 for \"not testing data\" or 1 for \"is testing data\". This is what we can feed into MAST-ML to explicitly define a set of Test data that is held out from all training."]},{"cell_type":"code","execution_count":45,"metadata":{"id":"Q4FJiTWAQt_K"},"outputs":[],"source":["mastml_df_clean[\"testdata\"]=0"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"crO7RhFwQcJf"},"outputs":[],"source":["for idx in test_indices:\n","  mastml_df_clean.at[idx,'testdata']=1"]},{"cell_type":"markdown","metadata":{"id":"ac80nDP6m_5c"},"source":["And with that setup we'll save a new version of the dataset just to keep track of changes as we go."]},{"cell_type":"code","execution_count":47,"metadata":{"id":"S4wpDg0zJAQN"},"outputs":[],"source":["output_path = \"./bandgap_data_v3.csv\"\n","mastml_df_clean.to_csv(output_path,index=False)"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"Hd9gB-VCnhcn"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>chemicalFormula Clean</th>\n","      <th>index</th>\n","      <th>Band gap values Clean</th>\n","      <th>Reliability</th>\n","      <th>testdata</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ag1Br1</td>\n","      <td>808.5</td>\n","      <td>3.485</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Ag1Cl1</td>\n","      <td>793.5</td>\n","      <td>4.190</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Ag1N3</td>\n","      <td>783.0</td>\n","      <td>3.900</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Ag1Te1</td>\n","      <td>820.0</td>\n","      <td>0.850</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Ag2O1</td>\n","      <td>785.0</td>\n","      <td>1.200</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  chemicalFormula Clean  index  Band gap values Clean  Reliability  testdata\n","0                Ag1Br1  808.5                  3.485          1.0         0\n","1                Ag1Cl1  793.5                  4.190          1.0         0\n","2                 Ag1N3  783.0                  3.900          1.0         0\n","3                Ag1Te1  820.0                  0.850          1.0         0\n","4                 Ag2O1  785.0                  1.200          1.0         1"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["mastml_df_clean.head(5)"]},{"cell_type":"markdown","metadata":{"id":"AEQqAql19ybB"},"source":["Notice how in the initial data cleaning and configuration there is still a bit that we do outside of MAST-ML. While MAST-ML gives a good deal of flexibility and useful tools for performing these machine learning workflows there will often still be custom steps like this that get added to the overall workflow that varies dataset by dataset."]},{"cell_type":"markdown","metadata":{"id":"fcWk1k177SWM"},"source":["## Section 3: Initializing MAST-ML\n","---\n","Now we'll dive into interacting more directly with the MAST-ML software. The first thing we need to do is setup some of the baseline information that MASTML will use as we call different sections of the code. This is similar to the [general] section from the previous configuration file oriented code base.\n"]},{"cell_type":"markdown","metadata":{"id":"dsRqGRLs_IXU"},"source":["Set the name of the savepath to save MAST-ML results to. It's recommended to make this a unique name each time you come back to this notebook. That way all the outputs you get from each session will be in a unique location that's easier to come back to later.\n","\n","By default I've set the output to the \"nanohub_workflow\" folder under our colab folder."]},{"cell_type":"code","execution_count":49,"metadata":{"id":"Xf8MFHjlM7Oe"},"outputs":[],"source":["SAVEPATH = './results'\n","\n","mastml = Mastml(savepath=SAVEPATH)\n","savepath = mastml.get_savepath"]},{"cell_type":"markdown","metadata":{"id":"I359kFP2_wpk"},"source":["With MAST-ML initialized you should see your output directory created. You can check this using the file tree on the left of the screen or directly through google drive.\n","\n","Next up we need to define the configuration of our Data file that we setup earlier. We'll define the names for all of the key components:  \n","target: the target variable that we want to predict  \n","extra_columns: the metadata columns that aren't features but we still want to keep track off  \n","testdata_columns: the column with binary values defining what is and isn't test data  \n","group_column: column names specifying unique groups in the data. We don't use this during this workflow  \n","as_frame: determines the structure of outputs. True gives us dataframe outputs that are easier to read in the notebook"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"kWxrG-RgxeTA"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING: feature_names not specified but target was specified. Assuming all columns except target and extra columns are features\n"]}],"source":["target = 'Band gap values Clean'\n","extra_columns = ['index', 'Reliability','chemicalFormula Clean']\n","testdata_columns = ['testdata']\n","\n","# calling the LocalDatasets section of the code initializes this section which we then execute with the method below\n","d = LocalDatasets(file_path='./bandgap_data_v3.csv',\n","                  target=target,\n","                  extra_columns=extra_columns,\n","                  group_column=None,\n","                  testdata_columns=testdata_columns,\n","                  as_frame=True)\n","\n","# Load the data with the load_data() method\n","data_dict = d.load_data()"]},{"cell_type":"markdown","metadata":{"id":"S07AJOwqBZbc"},"source":["Let's take a second to look through what just happened. In the previous cell the \"data_dict\" object was defined. It is a dictionary of various things that were loaded in from the dataseet. We'll pull those out of the dictionary to set them all to unique objects."]},{"cell_type":"markdown","metadata":{"id":"XT7AuA-mBuh9"},"source":["We see there are 5 keys:  \n","  X: the X feature matrix (used to fit the ML model). notice this is empty becausee we haven't done any feature generation  \n","  y: the y target data vector (true values)  \n","  X_extra: matrix of meta data not used in fitting (i.e. not part of X or y)  \n","  groups: vector of group labels. empty because we didn't set it  \n","  X_testdata: matrix or vector of left out data indices"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"Nrx-OGW3_dap"},"outputs":[{"data":{"text/plain":["dict_keys(['X', 'y', 'groups', 'X_extra', 'X_testdata'])"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["data_dict.keys()"]},{"cell_type":"markdown","metadata":{"id":"VBv3GmAaoKic"},"source":["We'll take each of these and assign them to their own object, to use going forwards."]},{"cell_type":"code","execution_count":52,"metadata":{"id":"a-Lrm2lG_7hi"},"outputs":[],"source":["X = data_dict['X']\n","y = data_dict['y']\n","X_extra = data_dict['X_extra']\n","groups = data_dict['groups']\n","X_testdata = data_dict['X_testdata']"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"pR-pTG-Zx-AW"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["Empty DataFrame\n","Columns: []\n","Index: [0, 1, 2, 3, 4]"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["X.head(5)"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"DDRPN14syAue"},"outputs":[],"source":["groups"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"VuKB49u_yM9c"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>Reliability</th>\n","      <th>chemicalFormula Clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>808.5</td>\n","      <td>1.0</td>\n","      <td>Ag1Br1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>793.5</td>\n","      <td>1.0</td>\n","      <td>Ag1Cl1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>783.0</td>\n","      <td>1.0</td>\n","      <td>Ag1N3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>820.0</td>\n","      <td>1.0</td>\n","      <td>Ag1Te1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>785.0</td>\n","      <td>1.0</td>\n","      <td>Ag2O1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   index  Reliability chemicalFormula Clean\n","0  808.5          1.0                Ag1Br1\n","1  793.5          1.0                Ag1Cl1\n","2  783.0          1.0                 Ag1N3\n","3  820.0          1.0                Ag1Te1\n","4  785.0          1.0                 Ag2O1"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["X_extra.head(5)"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"_jdQjcY5yN6z"},"outputs":[{"data":{"text/plain":["[array([  4,  20,  22,  30,  33,  41,  57, 111, 120, 121, 139, 141, 146,\n","        147, 163, 168, 192, 210, 214, 216, 223, 229, 250, 278, 279, 280,\n","        292, 300, 301, 324, 340, 341, 356, 366, 379, 393, 407, 415, 416,\n","        420, 422, 430, 436, 453, 454, 455, 460], dtype=int64)]"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["X_testdata"]},{"cell_type":"markdown","metadata":{"id":"Wg9RjtoaEP9Z"},"source":["## Section 4: Reproducing Key Workflow Steps\n","---\n","Now we'll start to dive into reproducing the key workflow steps from the previous notebook. These are:  \n","1) Feature Generation  \n","2) Feature Engineering  \n","3) Model Assessment and Training  \n","4) Model Optimization  \n","5) Model Predictions"]},{"cell_type":"markdown","metadata":{"id":"Jj5kqXFTCHXl"},"source":["If the data contains missing values (this one doesn't), we can clean the data with the built in tools in MAST-ML, which corrects missing values and provides some basic analysis of the input data. Since there are no missing values the data cleaner will still output some useful plots and statistics of our input data."]},{"cell_type":"code","execution_count":57,"metadata":{"id":"YgQJg2fDyQRU"},"outputs":[],"source":["cleaner = DataCleaning()\n","X, y = cleaner.evaluate(X=X,\n","                        y=y,\n","                        method='imputation',\n","                        strategy='mean',\n","                        savepath=savepath)"]},{"cell_type":"markdown","metadata":{"id":"IzuXHap6CYgy"},"source":["Looking at the format of the DataCleaning section also highlights the key way we will interact with MAST-ML in this format. For each section of the code we want to use we'll initialize it using what's called a class name, in this case \"DataCleaning\", and then call the \"evaluate\" method to essentially run the code for that Class.\n","\n","Let's look through the outputs and compare them to some of the initial dataset analysis and compare to the previous Nanohub workflow. Open the \"histogram_target_values.png\" file in the newly created DataCleaning folder under our output directory. Compare back to the histogram we made in the previous notebook. Are they the same?\n","\n","This is the type of check we would do to make sure we aren't missing any data switching between the two platforms."]},{"cell_type":"markdown","metadata":{"id":"O0cJ9kJAFMyd"},"source":["Next is generating the elemental features used in the model. Just like the previous step we define the class of feature generation we want to use, and then call the evaluate method. Again results are output to a new folder with the name of the Class that was evaluated. The features are also added to the X object so we can continue to use them directly without having to read in from the generated files.\n","\n","You can see from the output that MAST-ML is also performing some basic feature engineering by dropping features that are missing values. This is the most basic way of handling missing values, and if we wanted to do something more complex later we could come back and use imputation to fill in those missing values instead."]},{"cell_type":"code","execution_count":60,"metadata":{"id":"4Si2f98-cY6u"},"outputs":[{"ename":"TypeError","evalue":"ElementalFeatureGenerator.__init__() got an unexpected keyword argument 'featurize_df'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generator \u001b[38;5;241m=\u001b[39m ElementalFeatureGenerator(featurize_df \u001b[38;5;241m=\u001b[39m X_extra[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchemicalFormula Clean\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      2\u001b[0m                       feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomposition_avg\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m                       remove_constant_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m X, y \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mevaluate(X \u001b[38;5;241m=\u001b[39m X,\n\u001b[0;32m      5\u001b[0m                           y \u001b[38;5;241m=\u001b[39m y,\n\u001b[0;32m      6\u001b[0m                           savepath \u001b[38;5;241m=\u001b[39m savepath)\n","\u001b[1;31mTypeError\u001b[0m: ElementalFeatureGenerator.__init__() got an unexpected keyword argument 'featurize_df'"]}],"source":["generator = ElementalFeatureGenerator(featurize_df = X_extra[\"chemicalFormula Clean\"],\n","                      feature_types='composition_avg',\n","                      remove_constant_columns=True)\n","X, y = generator.evaluate(X = X,\n","                          y = y,\n","                          savepath = savepath)"]},{"cell_type":"markdown","metadata":{"id":"4vvs0Gd6GEnu"},"source":["Using the cell block below with outputs the feature object directly compare the features generated to those in the previous workflow. Do we have the same total number?\n","\n","If they're different can you think of any reasons why?  \n","hint: mastml does some initial cleaning automatically on the features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8LrJ6AX7hrpM"},"outputs":[],"source":["X"]},{"cell_type":"markdown","metadata":{"id":"m5ITre4zHYxf"},"source":["Next we'll see one of the benefits of using MAST-ML in this new way. Currently we don't have the same method impelemented in MAST-ML to remove highly correlated features. Previously adding this in would have been a good deal of work. But because we're using MAST-ML in this interactive notebook environment we can add in our own feature engineering steps that aren't included in the MAST-ML software. Below I just copied over the code from the previous notebook to filter highly correlated features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YIQTsE-0HzQV"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PkM9M1aoGydV"},"outputs":[],"source":["features_corr_df = X.corr(method=\"pearson\").abs()\n","# Filter the features with correlation coefficients above 0.95\n","upper = features_corr_df.where(np.triu(np.ones(features_corr_df.shape), k=1).astype(bool))\n","to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n","X = X.drop(columns=to_drop)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8L0jiPv1H4Cc"},"outputs":[],"source":["X"]},{"cell_type":"markdown","metadata":{"id":"GXzjcRTxIJ7p"},"source":["Next up we perform the last feature engineering step, which was to normalize the features using scikit-learn's MinMaxScaler method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0RxphS-Zivb0"},"outputs":[],"source":["preprocessor = SklearnPreprocessor(preprocessor='MinMaxScaler', as_frame=True)\n","X = preprocessor.evaluate(X=X,\n","                          y=y,\n","                          savepath=savepath)"]},{"cell_type":"markdown","metadata":{"id":"UDtU0nbmIaUO"},"source":["With our features setup now we jump into training, and evaluating models. This section is a bit more complex as we're defining multiple things at once. Things we define at the top of the cell are:  \n","1) The model or models to use. These need to be in list format which is why you see them in square brackets.  \n","2) A potential feature selector. Here we don't use one to mirror the previous workflow.  \n","3) Assessment metrics. We specify a range of them.  \n","4) A Splitter to use. The splitter is the class that we'll call in the bottom half and defines what kind of splits in the dataset we want to make. Recall that we previously established our Test set of data. So for this first test of the \"default model\" we don't need to make any additional splits which is why we use the NoSplit() class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ZryaRKb1cy-"},"outputs":[],"source":["default_decisiontree = SklearnModel(model='DecisionTreeRegressor')\n","models = [default_decisiontree]\n","metrics = ['r2_score', 'mean_absolute_error', 'root_mean_squared_error', 'rmse_over_stdev']\n","\n","splitter = NoSplit()\n","splitter.evaluate(X=X,\n","                  y=y,\n","                  models=models,\n","                  preprocessor=None,\n","                  metrics=metrics,\n","                  savepath=savepath,\n","                  X_extra=X_extra,\n","                  leaveout_inds=X_testdata,\n","                  verbosity=3)"]},{"cell_type":"markdown","metadata":{"id":"aGxkfJa-KIOO"},"source":["After this run completes we want to go look at how the model is performing. Navigate to the newly created \"DecisionTreeRegressor...\" folder and find both the \"parity_plot_leaveout.png\" file as well as the \"parity_plot_train.png\" file. Compare them both to eachother as well as to the parity plots made during the Nanohub notebook for the default model. Are they the same? Similar?  \n","\n","Note that the model type used is technically different. Previously we used a RandomForest with 1 tree which very closely mimics a sigle decision tree, and this time we explicitly used the decisiontreeregressor from scikit-learn."]},{"cell_type":"markdown","metadata":{"id":"lTe5_pQoyW8H"},"source":["Next we'll reproduce the model hyperparameter optimization we previously performed. Most things stay very similar but we switch to the Random Forest model so we can increase the number of trees again, and we need to add a new option to the splitter evaluate call which is the \"GridSearch\" class in mastml. This mirrors the same gridsearchcv call that is made in the previous nanohub notebook, however the format is slightly different.  \n","\n","For the GridSearch class we need to specify:  \n","1) param_names: the hyperparameters to grid over  \n","2) param_values: a string which specifies the grid.\n","\n","This follows the format of a linspace or logspace command in programs like matlab, or python packages like Numpy. the numbers in the string specify the starting value, ending value, and then number of points in between. Then we can give two options after which are lin/log which specifies whether the numbers are in linear space or log space. log space means we are specifying the exponent (10^x). And the last option is the type of number with int or float being the two most common versions. Some Hyperparametes need to be integers.\n","\n","3) scoring: a string specifying the score function to use."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rK89SsGzLvAe"},"outputs":[],"source":["default_RF = SklearnModel(model='Lasso')\n","models = [default_RF]\n","selector = [NoSelect()]\n","metrics = ['r2_score', 'mean_absolute_error', 'root_mean_squared_error', 'rmse_over_stdev']\n","grid1 = GridSearch(param_names='alpha',param_values='0 2 10 lin float',scoring='root_mean_squared_error')\n","grids = [grid1]\n","splitter = NoSplit()\n","splitter.evaluate(X=X,\n","                  y=y,\n","                  models=models,\n","                  preprocessor=None,\n","                  selectors=selector,\n","                  metrics=metrics,\n","                  savepath=savepath,\n","                  X_extra=X_extra,\n","                  leaveout_inds=X_testdata,\n","                  hyperopts = grids,\n","                  recalibrate_errors = True,\n","                  verbosity=3)\n"]},{"cell_type":"markdown","metadata":{"id":"JkeTDgt60PPz"},"source":["With the optimization run complete again we'll look through our outputs to find the results. Go into the new RandomForesRegressor folder and then into split_outer_0/split_0 and find the \"grid search\" files. One of them has the best identified hyperparameters, and the other give the full results for all options tried.\n","\n","Do the results match the previous gridsearch from the nanohub workflow? meaning do we get the same number of trees as the best?"]},{"cell_type":"markdown","metadata":{"id":"nqgMi85e_8DY"},"source":["The next step is to generate the 5-fold Cross Validation results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pqvdr6FYuUU-"},"outputs":[],"source":["default_decisiontree = SklearnModel(model='DecisionTreeRegressor')\n","models = [default_decisiontree]\n","selector = [NoSelect()]\n","metrics = ['r2_score', 'mean_absolute_error', 'root_mean_squared_error', 'rmse_over_stdev']\n","\n","splitter = SklearnDataSplitter(splitter='RepeatedKFold', n_repeats=2, n_splits=5)\n","splitter.evaluate(X=X,\n","                  y=y,\n","                  models=models,\n","                  preprocessor=None,\n","                  selectors=selector,\n","                  metrics=metrics,\n","                  savepath=savepath,\n","                  X_extra=X_extra,\n","                  leaveout_inds=X_testdata,\n","                  verbosity=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lMxWkitGoiEa"},"outputs":[],"source":["model = SklearnModel(model='Lasso',alpha=0.1)\n","model1 = SklearnModel(model='RandomForestRegressor',n_estimators=100)\n","model2 = SklearnModel(model='Ridge',alpha=0.1)\n","models = [model,model1,model2]\n","selector = [NoSelect()]\n","metrics = ['r2_score', 'mean_absolute_error', 'root_mean_squared_error', 'rmse_over_stdev']\n","\n","splitter = SklearnDataSplitter(splitter='RepeatedKFold', n_repeats=2, n_splits=5)\n","splitter.evaluate(X=X,\n","                  y=y,\n","                  models=models,\n","                  preprocessor=None,\n","                  selectors=selector,\n","                  metrics=metrics,\n","                  savepath=savepath,\n","                  X_extra=X_extra,\n","                  leaveout_inds=X_testdata,\n","                  recalibrate_errors = True,\n","                  verbosity=3)"]},{"cell_type":"markdown","metadata":{"id":"N27RMJB9KwZZ"},"source":["For the predictions section in the Nanohub workflow we used the test data as data to predict. In the MASTML framework all of the predictions are made and named with the convention \"leaveout...\" on the files. To compare how the test data is predicted between MASTML and the previous Nanohub notebook we can compare the predictions in these files to the ones made previously after optimizing the model."]},{"cell_type":"markdown","metadata":{"id":"w4qT_HLFAkHY"},"source":["## Section 5: Modifying the Workflow\n","---\n","\n","And with that we've completed the same steps as previously, using the MASTML code. In doing so we've been able to automatically generate our parity plots, along with a lot of other statistics and plots that we haven't learned about yet.\n","\n","And with this setup we can now do the last step of the activity, in which we can take advantage of the steps we've already established to start to make changes.\n","\n","In the current code we've used the DecisionTreeRegressor and RandomForestRegressor models from scikit-learn. Now choose another model type and repeat the workflow by modifying each step:\n","\n","1) pick another model type from scikit-learn. You can see a reference for available models here: https://scikit-learn.org/stable/supervised_learning.html\n","If you're not sure what kind of model to try I might suggest one of the linear type models such as Ridge Regression or LASSO. To see the list of available hyperparameters for each model you can click their respective link.\n","\n","2) build a default model where you don't change any hyperparameters from the scikit-learn defaults and analyze it's performance both on the Test data and with a 5-fold CV\n","\n","3) perform a grid search on 1 of the hyperparameters. I'd suggest picking the alpha hyperparameters if using one of the linear models suggested above.\n","\n","4) Compare the performance with the optimized hyperparameters. Were you able to improve the performance? how much did the RMSE value decrease for the Test set? How about the 5-fold CV test?"]},{"cell_type":"markdown","metadata":{"id":"FuI3KijtoTDJ"},"source":["copy code blocks from the previous sections of code to create your new workflow below. Step 1 is included to give an idea about the type of substitutions we can make (each should only be 1-2 lines of code)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j_g1tcIRBJ2q"},"outputs":[],"source":["X"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8C7WpP0sBphS"},"outputs":[],"source":["# this is step 1 above\n","default_LASSO = SklearnModel(model='Ridge',alpha=1.0) # < - here's my one line change that swaps to a LASSO model.\n","models = [default_LASSO]\n","selector = [NoSelect()]\n","metrics = ['r2_score', 'mean_absolute_error', 'root_mean_squared_error', 'rmse_over_stdev']\n","processor = SklearnPreprocessor(preprocessor='StandardScaler', as_frame=True)\n","\n","splitter = SklearnDataSplitter(splitter='RepeatedKFold', n_repeats=2, n_splits=5)\n","splitter.evaluate(X=X,\n","                  y=y,\n","                  models=models,\n","                  preprocessor=processor,\n","                  selectors=selector,\n","                  metrics=metrics,\n","                  savepath=savepath,\n","                  X_extra=X_extra,\n","                  leaveout_inds=X_testdata,\n","                  verbosity=3)"]}],"metadata":{"colab":{"collapsed_sections":["P7s6xyjq6zxv","fcWk1k177SWM"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
